---
title: 'Class 14 Structured random effects: time series and related models'
author: Andrew Parnell\newline \texttt{andrew.parnell@mu.ie} \newline \vspace{1cm}
  \newline \includegraphics[width=5cm]{MU_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: header.tex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
#setwd('/Volumes/MacintoshHD2/GitHub/ecots/slides/day_4/')
pkgs = c('rstan')
lapply(pkgs, library, character.only = TRUE)
rstan_options(auto_write = TRUE)
#options(mc.cores = parallel::detectCores())
```

## Learning outcomes

- Fit some time series models in Stan and use these with hierarchical models
- Some new methods
    
    - Autoregressive models
    - Stochastic Volatility Models
    - State space and dynamic models
    - A repeated measures time series

- Do some model comparison with Stan
- Show we can do shrinkage rather than model selection

## A short introduction to time series methods

\Large

Almost all of time series is based on two ideas:

1. Base your future predictions on previous values of the data
1. Base your future predictions on how wrong you were in your past predictions

Everything else in time series is just an extension of these!

In this class we will only discuss _discrete time series_, i.e. where $t = 1, 2, 3, ...$

## Decomposing time series

- We decompose time series commonly as:
$$y_t = \mbox{trend}_t + \mbox{seasonality}_t + \mbox{error}_t$$

- ... but sometimes it is not easy to separate these into different parts

- The concept of _stationarity_ helps us decompose the time series

## A time series with a big trend?

```{r, echo=FALSE}
set.seed(123)
y <- cumsum(sample(c(-1, 1), size=500, replace=TRUE))
plot(y, type = 'l')
```

## Generating the series

```{r, echo=TRUE}
set.seed(123)
y <- cumsum(sample(c(-1, 1), size=1000, replace=TRUE))
```

- The `sample` command just produces a set of 1000 values either -1 or 1
- `cumsum` just cumulatively adds them up
- This is a _random walk_ series

## Autoregressive (AR) models

- Autoregressive models literally perform a linear regression of the time series against the previous lag of the series

- For example, an AR(1) process can be written as:
$$y_{t} = \alpha + \beta y_{t-1} + \epsilon_{t}$$

- where $\epsilon_t \sim N(0, \sigma^2)$ just like a linear regression. 

- In a probability distribution format, we might write:
$$y_t \sim N(\alpha + \beta y_{t-1}, \sigma^2)$$
... and maximise the likelihood as normal

## Interpretation of the AR parameters

- $\alpha$ is an estimate of the stable mean of the process
- $\beta$ is interesting:

    - Values close to 1 indicate that the series is almost like a random walk. 
    - Values close to 0 indicate that the series is almost completely composed of random normally-distributed error terms
    - Values less than 0 indicate that the series is 'repulsive'
    - Values greater than 1 (or less than -1) indicate that the series is chaotic

## Fitting an AR model in `rstan`

```{r, eval = FALSE}
stan_code = '
...
model {
  for (t in 2:N) 
    y[t] ~ normal(alpha + beta * y[t-1], sigma);
  // Priors
  alpha ~ normal(0, 10);
  beta ~ normal(0, 10);
  sigma ~ uniform(0, 100);
}'
```

```{r, include = FALSE}
stan_code = '
data {
  int<lower=0> N; // number of observations
  vector[N] y; // response variable
}
parameters {
  real alpha; // intercept
  real beta; // AR parameter
  real<lower=0> sigma; // residual sd
}
model {
  for (t in 2:N) 
    y[t] ~ normal(alpha + beta * y[t-1], sigma);
  // Priors
  alpha ~ normal(0, 10);
  beta ~ normal(0, 10);
  sigma ~ uniform(0, 100);
}'
```

## Forest fire data

```{r, fig.height = 6}
ff = read.csv('../data/forest_fires.csv')
with(ff, plot(year, acres, type = 'l',
                ylab = 'Number of acres',
                xlab = 'Year'))
```

## Another way of running stan

\tiny 

- Set up a stan model
```{r, message=FALSE, results='hide'}
stan_mod_ar1 = stan_model(model_code = stan_code)
```

- Now choose either full MCMC...
```{r, eval = FALSE}
stan_run_ar1 = sampling(stan_mod_ar1,
                        data = list(y = scale(ff$acres)[,1],
                                    N = nrow(ff)))
```

- ...or just optimizing:
```{r, message = FALSE, results = 'hide'}
stan_opt_ar1 = optimizing(stan_mod_ar1,
                          data = list(y = scale(ff$acres)[,1],
                                      N = nrow(ff)))
```
```{r}
print(stan_opt_ar1)
```

## Changing the variance instead

- The AR model has a mean that changes but the variance is constant:
$$y_t \sim N(\alpha + \beta y_{t-1}, \sigma^2)$$
- Instead we could try:
$$y_t \sim N(\alpha, \sigma_t^2)$$

- Lots of different ways to model this:
  
    - Autoregressive Conditional Heteroskedasticity (ARCH) 
    - Generalised Autoregressive Conditional Heteroskedasticity (GARCH) 
    - Stochastic Volatility Models (SVM)

- They follow the same principles as AR models, but work on the standard deviations or variances instead of the mean

## Stochastic Volatility Modelling

- A Stochastic Volatility Model (SVM) models the variance as its own _stochastic process_
- The general model structure is often written as:
$$y_t \sim N( \alpha, \exp( h_t ) )$$
$$h_t \sim N( \mu + \phi h_{t-1}, \sigma^2)$$
- You can think of an SVM being like a GLM but with a log link on the variance parameter

## Mixing up models

- What if we wanted to fit an AR(1) model with stochastic volatility
- Impossible in almost any R package
- Simple to do in Stan!

## Code for a an AR(1)-SVM

\small
```{r}
stan_code = '
data {
  int<lower=0> N; // number of observations
  vector[N] y; // response variable
}
parameters {
  real alpha; // intercept
  real beta; // AR parameter
  vector[N] h; // stochastic volatility process
  real alpha_h; // SVM mean
  real beta_h; // SVM AR parameter
  real<lower=0> sigma_h; // SVM residual SD
}
model {
  h[1] ~ normal(alpha_h, 1);
  for (t in 2:N) {
    y[t] ~ normal(alpha + beta * y[t-1], sqrt(exp(h[t])));
    h[t] ~ normal(alpha_h + beta_h * h[t-1], sigma_h);
  }
}'
```

## Find the posterior distribution

\small 
```{r, include = FALSE}
stan_run_svm = stan(data = list(y = scale(ff$acres)[,1],
                                N = nrow(ff)),
                model_code = stan_code)
```
```{r}
print(stan_run_svm)
```

## Plot the important parameters

```{r, echo = FALSE, message = FALSE, results = 'hide'}
plot(stan_run_svm, pars = c('alpha', 'beta', 'alpha_h', 'beta_h', 'sigma_h'))
```

## Plot the $\sqrt{\exp(h)}$ values

```{r}
h_post = summary(stan_run_svm, pars = c("h"))$summary[,'50%']
plot(ff$year, sqrt(exp(h_post)), type = 'l')
```

## A repeated measures example

- Let's return to the Geese example all the way back on day 1:

```{r, echo = FALSE}
geese = read.csv('../data/geese_isotopes.csv')
geese$date = as.Date(paste(geese$year, geese$month ,geese$day, sep = '-'))
geese$int_days = round(geese$julianday)
o = order(geese$int_days)
with(geese, plot(int_days[o], scale(d13CPl[o])[,1],
                                  ylab = 'd13C',
                 xlab = 'Day of year'))
```


## What model would we like for these data?

- We have _repeated measures_ - more than one observation at each time point. 

- We would like the model to fill in the gaps and separate out the uncertainty due to the change over time from the uncertainty to do with repeated measurement

- We have to separate out the model into two layers:

    1. The observations and how they link to a single time series value on that day
    1. The underlying time series model defined at each time point
    
- A possible model:
$$y_t \sim N(\mu_{\mbox{day}_t}, \sigma^2)$$
$$\mu_{\mbox{day}} \sim N(\mu_{\mbox{day}-1}, \sigma_\mu^2)$$

    
## Stan code for a repeated measures random walk model

\tiny

```{r}
stan_code_rm = '
data {
  int<lower=0> N; // number of observations
  int<lower=0> N_day; // total number of days
  vector[N] y; // response variable
  int day[N]; // variable to match days to observations
}
parameters {
  real<lower=0> sigma; // st dev within day
  real<lower=0> sigma_mu; // st dev of RW
  vector[N_day] mu; // repeated measure parameter
}
model {
  mu[1] ~ normal(0, sigma_mu);
  for(t in 2:N_day) {
    mu[t] ~ normal(mu[t-1], sigma_mu);
  }
  sigma ~ uniform(0, 10);
  sigma_mu ~ uniform(0, 10);
  for (i in 1:N)
    y[i] ~ normal(mu[day[i]], sigma);
}'
```

## Optimise the parameters

```{r, include = FALSE}
stan_run_rm = stan(data = list(y = scale(geese$d13CPl)[,1],
                               N_day = 365,
                               N = nrow(geese),
                               day = geese$int_days),
                   model_code = stan_code_rm)
```
```{r}
print(stan_run_rm, pars = c('sigma_mu', 'sigma'))
```

## Plot the interesting parameters

```{r, echo = FALSE, message = FALSE, results = 'hide'}
plot(stan_run_rm, pars = c('sigma_mu', 'sigma'))
```

## Plot the best fit model

```{r, fig.height = 4}
with(geese, plot(int_days[o], scale(d13CPl[o])[,1],
                                  ylab = 'd13C',
                 xlab = 'Day of year'))
mu_post = summary(stan_run_rm, pars = c("mu"))$summary[,c('25%','50%','75%')]
lines(1:365, mu_post[,1], col = 'red', lty = 2)
lines(1:365, mu_post[,2], col = 'red', lty = 1)
lines(1:365, mu_post[,3], col = 'red', lty = 2)
```

## Shrinkage and AR models

- It's possible to do variable selection with AR models of multiple orders

- Suppose we want to choose the order of auto-regression $p$ in an AR(p) model

- We would fit a model for a large number of $p$ values and put a prior to reduce the size on the coefficients on them

- The normal isn't the only choice, an even more popular one is the double exponential (or Laplace) distribution

## Reminder: wheat data

```{r, echo = FALSE}
wheat = read.csv('../data/wheat.csv')
with(wheat, plot(year, wheat, type = 'l',
                 ylab = 'Wheat production (tonnes)',
                 xlab = 'Year'))
```

## Fitting a shrinkage AR model

\tiny
```{r}
stan_code_ar_shrink = '
data {
  int<lower=0> N; // number of observations
  int<lower=0> max_P; // maximum number of AR lags
  vector[N] y; // response variable
}
parameters {
  real alpha; // intercept
  vector[max_P] beta; // AR parameter
  real<lower=0> sigma; // residual sd
}
model {
  for (t in (max_P+1):N) {
        real mu;
        mu = alpha;
        for(k in 1:max_P)
          mu = mu + beta[k] * y[t-k];
        y[t] ~ normal(mu, sigma);
  }
  // Priors
  alpha ~ normal(0, 10);
  for (k in 1:max_P) {
    beta ~ double_exponential(0, 1);  
  }
  sigma ~ uniform(0, 100);
}'
```

## Fitting the model

```{r, include = FALSE}
stan_run_ar_shrink = stan(data = list(y = scale(wheat$wheat)[,1],
                                   N = nrow(wheat),
                                   max_P = 10),
                   model_code = stan_code_ar_shrink)
```
```{r}
print(stan_run_ar_shrink)
```

## Plot the AR parameters

```{r, message = FALSE, results = 'hide'}
plot(stan_run_ar_shrink, pars = c('beta'))
```

## Mixing up state space models, multivariate time series, Gaussian processes

- We can extend the simple state space model we met earlier to work for multivariate series
- We would have a state equation that relates our observations to a multivariate latent time series (possibly of a different dimension)
- We could change the time series model of the latent state to be an ARIMA model, an O-U process, a Gaussian process, or anything else you can think of!

## Dynamic linear models

- So far in all our models we have forced the time series parameters to be constant over time
- In a _Dynamic Linear Model_ we have a state space model with :
$$ y_t = F_t x_t + \epsilon_t,\; \epsilon_t \sim MVN(0, \Sigma_t)$$
$$ x_t = G_t x_{t-1} + \gamma_t,\; \gamma_t \sim N(0, \Psi_t)$$
- The key difference here is that the transformation matrices $F_t$ and $G_t$ can change over time, as can the variance matrices $\Sigma_t$ and $\Psi_t$, possibly in an ARCH/GARCH type framework
- These are very hard models to fit in JAGS/Stan but simple versions can work

## Latent factor time series models

- If we have very many series, a common approach to reduce the dimension is to use Factor Analysis or Principal components
- In a latent factor model we write:
$$y_t = B f_t + \epsilon_t$$
where now $B$ is a $num series \times num factors$  factor loading matrix which transforms the high dimensional $y_t$ into a lower dimensional $f_t$. 
- $f_t$ can then be run using a set of univariate time series, e.g. random walks 
- The $B$ matrix is often hard to estimate and might require some tight priors

## Summary

- Time series analysis involves using regression-type models to predict new data from previous time points
- With a Bayesian model you can do the model selection inside the modelling step
- Some really cool and flexible models can be fitted with advanced tools like state space models
