---
title: 'Class 6: Generalised linear mixed models'
author: Andrew Parnell\newline \texttt{andrew.parnell@mu.ie} \newline \vspace{1cm}
  \newline \includegraphics[width=5cm]{MU_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: header.tex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
#options(width = 40)
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
# setwd("~/GitHub/ecots/slides/day_1")
# pkgs = c('R2jags','rjags', 'lubridate', 'tidyverse','forecast', 'rstan')
# lapply(pkgs, library, character.only = TRUE)
library(lme4)
library(lattice)
```

## Learning outcomes

- Understand the basics of a glmm
- See a few different examples of glmms
- Understand how to fit basic glmms in `lme4`

## Revision: generalised linear models

- Recall that the normal linear model has residuals which are normally distributed and a response variable that is conditionally normally distributed
- e.g. $y_i = \alpha + \beta x_i + \epsilon_i$ with $\epsilon_i \sim N(0, \sigma^2)$ and $y_i | x_i \sim N(\alpha + \beta x_i, \sigma^2)$
- In a generalised linear model the residuals often don't exist and the response variable has a non-normal conditional distribution, e.g. Binomial, Poisson or Gamma (among many many others)
- The key to a glm is that the mean of the conditional distribution is transformed in a clever way via a _link_ function, and the transformed mean is given a standard linear relationship with the covariates

## Glm example

- Example 1: Binomial
$$y_i|x_i \sim Bin(N, p_i);\; logit(p_i) = \alpha + \beta x_i$$
- Example 2: Poisson
$$y_i|x_i \sim Po(\lambda_i);\; \log(\lambda_i) = \alpha + \beta x_i$$
- Example 3: Negative Binomial
$$y_i | x_i \sim NegBin(\phi, p_i);\; logit(p_i) = \alpha + \beta x_i$$

## Some important notes about glms

- Sometimes you have the choice as to how you collect your data (e.g. collecting precise numbers as opposed to yes/no values). When you have this choice, it is almost always preferable to collect the precise values as these will give you more precise results later on. Estimating the values of $\alpha$ and $\beta$ in e.g. a Binomial glm is often much less precise than estimating them in a normal linear model
- There are lots of different link functions with no strong guidance as to which one you should choose. For example some people use _probit_ instead of logit for Binomial models, and some people don't use any link function at all
- In the frequentist world, the fitting method becomes even more complicated when dealing with glms, often using something called Iteratively Re-weighted Least Squares (IRLS) which you might see referred to by `lme4`

## Example: the swiss willow tit data

\tiny
```{r}
swt = read.csv('../data/swt.csv')
head(swt)
```

## A first glm

- Suppose we want to fit a model on the sum $y_i =$ `rep.1 + rep.2 + rep.3`:
$$y_i \sim Bin(N_i, p_i), logit(p_i) = \alpha + \beta (x_i- \bar{x})$$
where $x_i$ is the percentage of forest cover

- There are no random effects in this (yet) so we currently have just a glm 

- Remember that the relationship between $x_i$ and $p_i$ (the probability of observing a bird) is not linear. People usually use $\exp(\beta)$ as a measure of the proportional increase in the probability associated with a unit increase in $x$

## Fitting the glm

```{r, echo = FALSE, message=FALSE, results = 'hide'}
sum_fun = function(x) {
  s = ifelse(is.na(x[1]),0,x[1]) + ifelse(is.na(x[2]),0,x[2]) + ifelse(is.na(x[3]),0,x[3])
  N = ifelse(is.na(x[1]),0,1) + ifelse(is.na(x[2]),0,1) + ifelse(is.na(x[3]),0,1)
  return(c(s,N))
}
y = apply(swt[,1:3],1,sum_fun)[1,]
N = apply(swt[,1:3],1,sum_fun)[2,]
x = swt$forest
```

\tiny
```{r}
summary(glm(cbind(y, N) ~ x, family = binomial(link = logit)))
```

## Changing to a glmm

- Now extend the model to have a random intercept by altitude
$$y_{ij} \sim Bin(N_{ij}, p_{ij}), logit(p_{ij}) = \alpha_j + \beta (x_i- \bar{x})$$
with $\alpha_j \sim N(\mu_\alpha, \sigma_\alpha^2)$.

- Now $y_{ij}$ is the count for observation $i$ at altitude $j$. Other parameters defined similarly

- This means that there will be three different $\alpha_j$ values for altitude low, medium and high

## Plot of the data

```{r}
xyplot(y/N ~ x|alt, swt, type='p',
       layout=c(1,3), index.cond = function(x,y)max(y))
```



## Fitting the glmm

\tiny
```{r}
glmm_1 = glmer(cbind(y, N) ~ x + (1 | alt),
              family = binomial, data = swt)
summary(glmm_1)
```

## Look at the random effects

```{r, fig.height = 6}
library(lattice)
dotplot(ranef(glmm_1))
```

## Plot the probabilities

```{r, fig.height = 6}
p_est = predict(glmm_1, type = 'response')
plot(x, y/N, col = swt$alt, las = 1)
points(x, p_est, col = swt$alt, pch = 19)
legend('topleft', c('high', 'low', 'medium'), pch = 19, col=1:3)
```

## A model with varying intercepts and slopes

\tiny
```{r}
glmm_2 = glmer(cbind(y, N) ~ x + (x | alt),
              family = binomial, data = swt)
summary(glmm_2)
```

## Plot the new random effects

```{r, fig.height = 6}
dotplot(ranef(glmm_2))
```

## Compare the two binomial models

```{r}
anova(glmm_1, glmm_2)
```

## A Poisson model - glm set-up

- If we wanted a Poisson glm we would set it up as:
$$y_i \sim Po(\lambda_i); \; \log(\lambda_i) = \alpha + \beta x_i$$

- $\log$ is the link function here. Again people often use $\exp(\beta)$ as an estimate of how the rate parameter $\lambda$ is affected by $x$

- The Poisson is a really un-realistic model. Remember it assumes that the mean and the variance of $y$ are the same. This almost never occurs in real data

## Poisson glm - example data

```{r, fig.height = 6}
horseshoe = read.csv('../data/horseshoe.csv')
xyplot(satell ~ weight|as.factor(color), horseshoe, 
       type='p',layout=c(1,4))
```

## Fit Poisson glm

\tiny
```{r}
summary(glm(satell ~ weight, data = horseshoe,family = poisson(link = log)))
```

## A Poisson glmm

\tiny
```{r}
glmm_3 = glmer(satell ~ weight + (1 | color),
              family = poisson, data = horseshoe)
summary(glmm_3)
```

## A different type of Poisson model

- As previously stated the Poisson is a bit unrealistic (because of mean = variance assumption)

- Random effects can be added in to model overdispersion:
$$y_i \sim Po(\lambda_i), \; \log(\lambda_i) = \alpha + \beta x_i + \epsilon_i$$
with $\epsilon_i \sim N(0, \sigma^2)$

- This is just adding an _individual-level_ random effect (or a residual term) and is the same as:
$$\log(\lambda_i) \sim N(\alpha + \beta x_i, \sigma^2)$$

## Poisson over-dispersion model

\tiny
```{r}
horseshoe$obs <- 1:nrow(horseshoe)
glmm_4 = glmer(satell ~ weight + (1 | obs),
              family = poisson, data = horseshoe,
              control = glmerControl(optimizer = "Nelder_Mead"))
summary(glmm_4)
```

## Model results

```{r}
anova(glmm_3, glmm_4)
```

Poisson OD model a far better fit

## Some notes on glmm

- Really just scratching the surface. Many many options for distributions and link functions

- Basic idea (and computational approach) is exactly the same as for `lmer`

- Individual level random effects are often useful in glmms as they can represent over-dispersion. They essentially just add a residual effect into the linked mean

## Summary

- We have seen a Binomial and a Poisson generalised linear mixed model (glmm)

- Very simple to fit in `lme4` using the `glmer` function. Exactly the same formula approach

- Over-dispersion a useful trick for getting good-fitting models