---
title: 'Class 11: Multivariate and multi-layer hierarchical models'
author: Andrew Parnell\newline \texttt{andrew.parnell@mu.ie} \newline \vspace{1cm}
  \newline \includegraphics[width=5cm]{MU_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: header.tex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_knit$set(global.par = TRUE)
set.seed(123)
```
```{r, include=FALSE}
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
```


## Learning outcomes:

- Understand how to add in multiple layers to a hierarchical model
- Follow a detailed example of building a model
- Be able to fit multivariate models in `rstan`

## Some new terminology

- Most of the models we have covered so far contain only one hidden or _latent_ set of parameters
- For example, the data $y$ may depend on a parameter $\beta$, which itself depends on a parameter $\theta$. $\theta$ is given a prior distribution
- We say that the data are at the 'top level', the parameter $\beta$ is a _latent parameter_ at the second level, and the hyper-parameter $\theta$ is also a latent parameter at the third level
- We say that the prior distribution on $\beta$ is _conditional_ on $\theta$, whilst the prior distribution (if it just involves numbers) is a _marginal prior distribution_

## What is a multi-layer model?

- A multi-layer model is one where have many (usually more than 2 or 3)  layers of parameters conditional on each other
- It's very straightforward to add in these extra layers in Stan
- The question is whether they are necessary or not, and how much the data can tell us about them

## A simple example

- We will work through the `earnings` example, extending it to produce a much more complex model
- Going back to the linear regression version, we had:

```{r, eval = FALSE}
  '
  ...
  y ~ normal(intercept + slope * x, residual_sd);
  ...
  intercept ~ normal(0, 100);
  slope ~ normal(0, 100);
  residual_sd ~ cauchy(0, 10);
  '
```

## Adding extra layers

\tiny
```{r}
stan_code = '
data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real intercept;
  real slope;
  real<lower=0> residual_sd;
  real mu_intercept;
  real mu_slope;
  real sd_intercept;
  real sd_slope;
} 
model {
  // Likelihood
  y ~ normal(intercept + slope * x, residual_sd);
  // Priors
  intercept ~ normal(mu_intercept, sd_intercept);
  slope ~ normal(mu_slope, sd_slope);
  residual_sd ~ cauchy(0, 10);
  mu_intercept ~ normal(0, 2);
  mu_slope ~ normal(0, 2);
  sd_intercept ~ cauchy(0, 1);
  sd_slope ~ cauchy(0, 1);
}
'
```

## What information is there about these extra parameters?

```{r, include = FALSE}
earnings = read.csv('../data/earnings.csv')
library(rstan)
#options(mc.cores = parallel::detectCores())
stan_run = stan(data = list(N = nrow(earnings), 
                            y = earnings$y,
                            x = earnings$x_centered),
                model_code = stan_code)
```
```{r, echo = FALSE, fig.height = 5}
plot(stan_run)
```

## How many layers should I add?

- We could go on adding layers here if we wanted to, but it's not clear what benefit it would have
- For example, do we really need to know the standard deviation of the mean of the intercept? The answer to this will depend on the questions of interest, and the amount and type of prior information available
- A general rule is, in the absence of any strong prior information, to add one extra layer of parameters beyond the latent parameters of key interest

## Stan and indexing

- When writing out the model with differing slopes and intercepts for each ethnic group, the hierarchical formulation ties them together through the prior distributions
- The likelihood is written as:
```
    y[i] ~ normal(alpha[eth[i]] + 
                  beta[eth[i]]*(x[i] - mean(x)), 
                    sigma)
```
which in maths can be written as:
$$y_i \sim N(\alpha_{\mbox{eth}_i} + \beta_{\mbox{eth}_i} x_i, \sigma^2)$$
where $\mbox{eth}_i$ takes the values 1, 2, 3, or 4

- Remember, $y_i$ is the log-earnings of individual $i$ where $i=1,\ldots,N$

## Re-writing the model 

- Commonly you'll see $y$ here re-defined as $y_{ij}$ where $j=1,..,4$ represents ethnicity, and $i=1,\ldots,N_j$ is the number of individuals with ethnicity $j$
- The likelihood can then be written as:
$$y_{ij} \sim N(\alpha_j + \beta_j x_i, \sigma^2)$$
- Note that this is exactly the same model, just re-written slightly differently. In fact, this latter model is much harder to write out in Stan code

## Fixed vs random effect models

- Thinking about this model in more detail
$$y_{ij} \sim N(\alpha_j + \beta_j x_i, \sigma^2)$$
- If the $\alpha_j$ and $\beta_j$ parameters are all given independent prior distributions, e.g. $\alpha_j \sim N(0, 100)$ or similar, then this is considered a _fixed effects_ model
- If the $\alpha_j$ and $\beta_j$ are given prior distributions that tie the values together, e.g. $\alpha_j \sim N(\mu_\alpha, \sigma_\alpha^2)$, then this is often called a _random effects_ model
- (In fact, nobody can agree on what a fixed or random effects model actually is)
- If you have multiple different types of priors then this is called a _mixed effects_ model

## Mixed effects vs hierarchical models

- The hierarchical models we have been studying all use the _random effects_ approach wherever possible
- The big advantage of using this approach is that we get to _borrow_ strength between the different groups (here `eth`, but it could be anything)
- Whenever we have a categorical covariate we should always be putting a constraining/tying prior distribution on them, and looking at how the effects vary between the groups
- This is strongly linked to the idea of _partial pooling_ which we will meet later in the course

## Example: multi-layer earnings data

- We will now go through and build a much more complicated model for the earnings data, taken from the Gelman and Hill book, using only weak priors
- We can generate data from these models (either using the prior or the posterior), and we can draw a DAG
- Our goal is to explore the factors which explain earnings. We have variables on height, age, and ethnicity (but not sex!).
- If we first plot the data
```{r, echo = FALSE, fig.height=4}
dat = read.csv('../data/earnings.csv')
par(mfrow=c(1,2))
plot(jitter(dat$height_cm), dat$earn, xlab = 'Height (cm)', ylab = 'Earnings ($)')
plot(jitter(dat$height_cm), log(dat$earn), xlab = 'Height (cm)', ylab = 'log(Earnings ($))')
par(mfrow=c(1,1))
```

## Transformations

- From the left-hand plot there seem to be quite a few extreme observations, and there's a possibility that the relationship between height and earnings is non-linear
- The right-hand plot seems to have stabilised most of the extreme observations, and perhaps linearity is more appropriate
- Notice that a linear model implies:
$$y_i = \alpha + \beta x_i + \epsilon_i$$
whilst the log-linear model implies:
$$y_i = \exp( \alpha + \beta x_i + \epsilon_i) = e^\alpha \times e^{\beta x_i} \times e^\epsilon_i$$
so the coefficients, once exponentiated, have multiplicative effects
that are relatively easy to interpret

## Fitting the first model

- If we fit a model with just height (mean centered) we get the following Stan output

\tiny
```{r, include = FALSE}
stan_code_2 = '
data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real intercept;
  real slope;
  real<lower=0> residual_sd;
}
model {
  // Likelihood
  y ~ normal(intercept + slope * x, residual_sd);
  // Priors
  intercept ~ normal(0, 100);
  slope ~ normal(0, 100);
  residual_sd ~ cauchy(0, 10);
}
'
stan_run_2 = stan(data = list(N = nrow(earnings), 
                            y = earnings$y,
                            x = earnings$x_centered),
                model_code = stan_code_2)
```
```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.height=6}
plot(stan_run_2)
```

## Interpreting the parameters

- These parameters are directly interpretable:

\tiny
```{r}
stan_run_2_summ = summary(stan_run_2)
round(stan_run_2_summ$summary, 2)
```
```{r, echo = FALSE}
slope = stan_run_2_summ$summary['slope',1]
sig = stan_run_2_summ$summary['residual_sd',1]
```

  - The mean of the log earnings at the mean height is about 9.737, which is about 17k on the original scale
  - We can also get e.g. a 95% confidence interval directly from the output
  - For every extra cm so you gain $`r format(slope, scientific = FALSE, digits = 3)`$ on the log scale, i.e. an $`r format(100*exp(slope)-100, scientific = FALSE, digits = 3)`$% gain in income
  - From the posterior of $\sigma$, we can guess that about 68% of predictions will be within $`r format(sig, scientic = FALSE, digits = 3)`$ on the log scale or within a factor of about $`r format(exp(sig), scientic = FALSE, digits = 3)`$ of the prediction 
  - Interpretation for the intercept would have been harder had we not mean-centered the height variable

## Improving the model

- Now suppose we fit a model with a random intercept for ethnicity

\tiny
```{r}
stan_code_3 = '
data {
  int N;
  int N_eth;
  vector[N] x;
  vector[N] y;
  int eth[N];
}
parameters {
  real intercept[N_eth];
  real slope;
  real mean_intercept;
  real<lower=0> residual_sd;
  real<lower=0> sigma_intercept;
} 
model {
  // Likelihood
  for (i in 1:N) {
    y[i] ~ normal(intercept[eth[i]] + slope * x[i], residual_sd);
  }
  // Priors
  slope ~ normal(0, 0.1);
  for (j in 1:N_eth) {
    intercept[j] ~ normal(mean_intercept, sigma_intercept);
  }
  mean_intercept ~ normal(11, 2);
  sigma_intercept ~ cauchy(0, 10);
  residual_sd ~ cauchy(0, 10);
}
'
```

## Random intercept model fits

```{r, message =FALSE, warning= FALSE, results = 'hide', include = FALSE}
stan_run_3 = stan(data = list(N = nrow(earnings), 
                              y = earnings$y,
                              x = earnings$x_centered,
                              eth = earnings$eth,
                              N_eth = length(unique(earnings$eth))),
                model_code = stan_code_3)
```

```{r, echo = FALSE, fig.height = 6}
plot(stan_run_3)
```


## Interpreting the output

\tiny
```{r}
stan_run_3_summ = summary(stan_run_3)
round(stan_run_3_summ$summary, 2)
```

```{r, include = FALSE}
slope = stan_run_3_summ$summary['slope',1]
sig = stan_run_3_summ$summary['residual_sd',1]
```

- The parameters $\alpha$ and $\beta$ haven't changed much in the mean 
- The 95% confidence interval for $\alpha$ has increased
- We also have estimates for each ethnicity intercept, none of these have a strong effect away from the mean

## Model with varying slopes and intercepts

\tiny
```{r}
stan_code_4 = '
data {
  int N;
  int N_eth;
  vector[N] x;
  vector[N] y;
  int eth[N];
}
parameters {
  real intercept[N_eth];
  real slope[N_eth];
  real<lower=0> residual_sd;
  real mean_intercept;
  real<lower=0> sigma_intercept;
  real mean_slope;
  real<lower=0> sigma_slope;
} 
model {
  // Likelihood
  for (i in 1:N) {
    y[i] ~ normal(intercept[eth[i]] + slope[eth[i]] * x[i], residual_sd);
  }
  // Priors
  for (j in 1:N_eth) {
    intercept[j] ~ normal(mean_intercept, sigma_intercept);
    slope[j] ~ normal(mean_slope, sigma_slope);
  }
  mean_intercept ~ normal(11, 2);
  sigma_intercept ~ cauchy(0, 10);
  mean_slope ~ normal(0, 1);
  sigma_slope ~ cauchy(0, 10);
  residual_sd ~ cauchy(0, 10);
}
'
```

## Random intercept/slope model fits

```{r, message =FALSE, warning = FALSE, results = 'hide', include = FALSE}
stan_run_4 = stan(data = list(N = nrow(earnings), 
                              y = earnings$y,
                              x = earnings$x_centered,
                              eth = earnings$eth,
                              N_eth = length(unique(earnings$eth))),
                model_code = stan_code_4)
```

```{r, echo = FALSE, fig.height = 6}
plot(stan_run_4)
```

## Random intercept/slopes interpretation

\tiny
```{r}
stan_run_4_summ = summary(stan_run_4)
round(stan_run_4_summ$summary, 2)
```

- Some evidence of variability in the slopes
- `sigma_slope` in particular quite variable
- Still not much variability in intercepts

## Introducing age

- Let's fit an even more complicated model with intercepts and slopes varying by ethnicity and age group

- Age is divided up into three groups 1: 18-34, 2: 35-49, and 3: 50-64

- We want to know whether the degree to which height affects earnings for different ethnic/age group combinations

## Stan model

\tiny
```{r}
stan_code_5 = '
data {
  int N;
  int N_eth;
  int N_age;
  vector[N] x;
  vector[N] y;
  int eth[N];
  int age[N];
}
parameters {
  matrix[N_eth, N_age] intercept;
  matrix[N_eth, N_age] slope;
  real<lower=0> residual_sd;
  real mean_intercept;
  real<lower=0> sigma_intercept;
  real mean_slope;
  real<lower=0> sigma_slope;
} 
model {
  // Likelihood
  for (i in 1:N) {
    y[i] ~ normal(intercept[eth[i], age[i]] + slope[eth[i], age[i]] * x[i], residual_sd);
  }
  // Priors
  for (j in 1:N_eth) {
    for (k in 1:N_age) {
      intercept[j,k] ~ normal(mean_intercept, sigma_intercept);
      slope[j,k] ~ normal(mean_slope, sigma_slope);
    }
  }
  mean_intercept ~ normal(11, 2);
  sigma_intercept ~ cauchy(0, 10);
  mean_slope ~ normal(0, 1);
  sigma_slope ~ cauchy(0, 10);
  residual_sd ~ cauchy(0, 10);
}
'
```

## Random age/ethnicity model fits

```{r, message =FALSE, warning = FALSE, results = 'hide', include = FALSE}
stan_run_5 = stan(data = list(N = nrow(earnings), 
                              y = earnings$y,
                              x = earnings$x_centered,
                              eth = earnings$eth,
                              age = earnings$age,
                              N_eth = length(unique(earnings$eth)),
                              N_age = length(unique(earnings$age))),
                model_code = stan_code_5)
```

```{r, echo = FALSE, fig.height = 6}
plot(stan_run_5)
```

## Random intercept/slopes interpretation

\tiny
```{r}
stan_run_5_summ = summary(stan_run_5)
round(stan_run_5_summ$summary, 2)
```

- Lots of variability in slopes, especially `sigma_slope`
- Not much elsewhere


## Plotting the fitted values

```{r, echo = FALSE}
par_means = colMeans(as.matrix(stan_run_5))
par_names = names(par_means)
intercept_means = matrix(par_means[grep('intercept\\[', par_names)], 
                         4, 3)
slope_means = matrix(par_means[grep('slope\\[', par_names)], 
                         4, 3)
age_grp_names = c('18-34','35-49','50-64')
eth_names = c('Blacks','Hispanics','Whites','Others')
par(mfrow=c(4,3))
for(i in 1:4) {
  for(j in 1:3) {
    curr_dat = subset(dat, dat$eth == i & dat$age == j)
    plot(curr_dat$height_cm, log(curr_dat$earn), main = paste(eth_names[i], age_grp_names[j]), ylab = 'log(earnings)', xlab = 'Height (cm)')
    lines(dat$height_cm, intercept_means[i,j] + slope_means[i,j]*(dat$height_cm - mean (dat$height_cm)), col = i)    
  }
}
par(mfrow=c(1,1))
```

## More about this model

- So we now have varying effects - we should also plot the uncertainties in these lines (see practical)

- There are ways to make this model more complicated

    1. We could start partitioning up the nested random effects to have e.g. means associated with just age group or ethnicity
    
    1. We could start fitting multivariate random effects models ...

## Multivariate models

- Often we have more than one variable or parameter that changes simultaneously

- For example, we might be interested in how both earnings and social grade change in response to height. These are likely to be correlated, even after we take into account the effect of height

- Or we might be interested in how both the slope and the intercept change in a particular model

- In either case, the _multivariate normal distribution_ helps us achieve this by borrowing strength across the different dimensions

## The multivariate normal distribution

- The standard normal distribution we have met has two parameters, a mean and a standard deviation/variance
- The multivariate normal (MVN) distribution has the same, except that the mean is now a _vector_ and the variance is now a _matrix_ sometimes called the _covariance matrix_
- Each element of the mean represents the mean of each variable. Each diagonal element of the covariance matrix is the variance of that variable, and each off-diagonal element is the covariance between those two variables
- For the 2D MVN distribution, we write:
$$\left[ \begin{array}{c} y_1 \\ y_2 \end{array} \right] \sim N \left( \left[ \begin{array}{c} \mu_1 \\ \mu_2 \end{array} \right], \left[ \begin{array}{cc} v_{11} & v_{12} \\ v_{12} & v_{22} \end{array} \right] \right)$$
or, for short
$$y \sim MVN(\mu, \Sigma)$$

## Learning about the parameters of the MVN

- You can think of the MVN distribution as borrowing strength between variables, just like the hierarchical model borrows strength across categories
- If we want to use the MVN in a Bayesian model we need to be able to put prior distributions on the parameters
- The means $\mu$ are easier as we can set independent prior distributions on them, e.g. $\mu_1 \sim N(0, 100)$
- The covariance matrix is more fiddly. Luckily there is a probability distribution called the _Wishart distribution_ which works on covariance matrices. Stan also has some special probability distributions for covariance matrices

## Returning to the earnings data

- Let's add to our complicated model with intercepts and slopes varying by ethnicity and age group

- We will include a multivariate normal prior on the intercept/slope pairs and look at the interaction between ethnicity and ages

- We are introducing a specific parameter which measures the degree of correlation between the intercept and the slope for each group. This could potentially vary between age group or ethnicity

- We will show this model but we won't run it (too slow and inelegant)

## Fitting a multivariate, multi-layer model

\tiny
```{r}
stan_code_6 = '
data {
  int N;
  int N_eth;
  int N_age;
  vector[N] x;
  vector[N] y;
  int eth[N];
  int age[N];
}
parameters {
  vector[2] beta[N_eth, N_age];
  real<lower=0> residual_sd;
  vector[2] mean_beta;
  cov_matrix[2] Sigma_beta;
} 
model {
  // Likelihood
  for (i in 1:N) {
    y[i] ~ normal(beta[eth[i], age[i], 1] + beta[eth[i], age[i], 2] * x[i], residual_sd);
  }
  // Priors
  for (j in 1:N_eth) {
    for (k in 1:N_age) {
      beta[j,k] ~ multi_normal(mean_beta, Sigma_beta);
    }
  }
  for (l in 1:2) {
    mean_beta[l] ~ normal(0, 10);
  }
  residual_sd ~ cauchy(0, 10);
}
'
```

```{r, message =FALSE, warning = FALSE, results = 'hide', eval = FALSE, include = FALSE}
stan_run_6 = stan(data = list(N = nrow(earnings), 
                              y = earnings$y,
                              x = earnings$x_centered,
                              eth = earnings$eth,
                              age = earnings$age,
                              N_eth = length(unique(earnings$eth)),
                              N_age = length(unique(earnings$age))),
                model_code = stan_code_6)
```

## Model comparison with `rstan` models

- We have already seen that you can directly use the `loo` and `waic` functions in the `loo` package to get useful Bayesian model comparison values
- Unfortunately you can't use these directly in any code you write from Stan
- If you do want to use them you need to put some extra lines at the bottom of the R code
- This is because `WAIC` and `loo` require estimates of the log likelihood in order to calculate their values

## Regression example

\tiny 
```{r}
stan_code_loo = '
data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real intercept;
  real slope;
  real<lower=0> residual_sd;
} 
model {
  // Likelihood
  y ~ normal(intercept + slope * x, residual_sd);
  // Priors
  intercept ~ normal(0, 20);
  slope ~ normal(0, 1);
  residual_sd ~ cauchy(0, 10);
}
generated quantities {
  vector[N] log_lik;
  for (i in 1:N) {
    log_lik[i] = normal_lpdf(y[i] | intercept + slope * x[i], residual_sd);
  }
}
'
```

## Getting the loo and WAIC

\small
```{r, message=FALSE, warning=FALSE, results = 'hide'}
stan_run_loo = stan(data = list(N = nrow(earnings), 
                                y = earnings$y,
                                x = earnings$x_centered),
                model_code = stan_code_loo)
library(loo)
log_lik <- extract_log_lik(stan_run_loo)
```
```{r, message = FALSE, warning = FALSE}
loo(log_lik)
```

## Summary

- We have seen how to create some rich multi-layers models
- We have gone through quite a detailed example
- We have learnt about the multivariate normal distribution for even more complicated hierarchical models
- We have computed the `loo` and `waic` for a simple model
