---
title: 'Class 11: Multivariate and multi-layer hierarchical models'
author: Andrew Parnell\newline \texttt{andrew.parnell@mu.ie} \newline \vspace{1cm}
  \newline \includegraphics[width=5cm]{MU_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_knit$set(global.par = TRUE)
set.seed(123)
```
```{r, include=FALSE}
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
```


## Learning outcomes:

- Understand how to add in multiple layers to a hierarchical model
- Follow a detailed example of building a model
- Be able to fit multivariate models in JAGS
- Be able to work with missing data in JAGS

## Some new terminology

- Most of the models we have covered so far contain only one hidden or _latent_ set of parameters
- For example, the data $y$ may depend on a parameter $\beta$, which itself depends on a parameter $\theta$. $\theta$ is given a prior distribution
- We say that the data are at the 'top level', the parameter $\beta$ is a _latent parameter_ at the second level, and the hyper-parameter $\theta$ is also a latent parameter at the third level
- We say that the prior distribution on $\beta$ is _conditional_ on $\theta$, whilst the prior distribution (if it just involves numbers) is a _marginal prior distribution_

## What is a multi-layer model?

- A multi-layer model is one where have many (usually more than 2 or 3)  layers of parameters conditional on each other
- It's very straightforward to add in these extra layers in JAGS/Stan
- The question is whether they are necessary or not, and how much the data can tell us about them

## A simple example

- We will work through the `earnings` example, extending it to produce a much more complex model
- The model we last met had the following JAGS code:
```{r, eval = FALSE}
  '
  ...
  y[i] ~ dnorm(alpha[eth[i]] + 
                  beta[eth[i]]*(x[i] - mean(x)), 
                    sigma^-2)
  }
  ...
  # Priors
  for(j in 1:N_eth) {
    alpha[j] ~ dnorm(mu_alpha, sigma_alpha^-2)
    beta[j] ~ dnorm(mu_beta, sigma_beta^-2)
  }'
```

## Adding extra layers

- The priors for the hyper-parameters at the bottom of the model were
```{r, eval = FALSE}
'
  ...
  mu_alpha ~ dnorm(11, 2^-2)
  mu_beta ~ dnorm(0, 0.1^-2)
  sigma ~ dunif(0, 5)
  sigma_alpha ~ dunif(0, 2)
  sigma_beta ~ dunif(0, 2)
}
'
```

- There is nothing to stop us adding in extra parameters beneath these layers

## New JAGS code

\tiny
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    y[i] ~ dnorm(alpha[eth[i]] + 
                  beta[eth[i]]*(x[i] - mean(x)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    alpha[j] ~ dnorm(mu_alpha, sigma_alpha^-2)
    beta[j] ~ dnorm(mu_beta, sigma_beta^-2)
  }
  mu_alpha ~ dnorm(mu_alpha_0, sigma_alpha_0^-2)
  mu_alpha_0 ~ dnorm(0, 20^-2)
  sigma_alpha_0 ~ dt(0,5,1)T(0,)
  mu_beta ~ dnorm(mu_beta_0, sigma_beta_0^-2)
  mu_beta_0 ~ dnorm(0, 20^-2)
  sigma_beta_0 ~ dt(0,5,1)T(0,)
  sigma ~ dunif(0, 5)
  sigma_alpha ~ dt(0,5,1)T(0,)
  sigma_beta ~ dt(0,5,1)T(0,)
}
'
```

## What information is there about these extra parameters?

```{r, message=FALSE, results='hide', echo = FALSE}
library(R2jags)
dat = read.csv('../data/earnings.csv')
jags_run = jags(data = list(N = nrow(dat), 
                            y = log(dat$earn),
                            eth = dat$eth,
                            N_eth = length(unique(dat$eth)),
                            x = dat$height_cm),
                parameters.to.save = c('mu_alpha_0',
                                       'sigma_alpha_0',
                                       'mu_beta_0',
                                       'sigma_beta_0'),
                model.file = textConnection(jags_code))
```
```{r, echo = FALSE}
pars = jags_run$BUGSoutput$sims.matrix
par(mfrow=c(2,2))
for(i in 1:4) {
  hist(pars[,i+1], main= colnames(pars)[i+1], xlab = '', breaks = 30)
}
par(mfrow=c(1,1))
```

## How many layers should I add?

- We could go on adding layers here if we wanted to, but it's not clear what benefit it would have
- For example, do we really need to know the standard deviation of the mean of the intercept? The answer to this will depend on the questions of interest, and the amount and type of prior information available
- A general rule is, in the absence of any strong prior information, to add one extra layer of parameters beyond the latent parameters of key interest

## Writing the same model in different ways

- The model on the previous slides has a different intercept and slope for each ethnicity group, with the information about them tied together through the prior distributions on them
- The likelihood was written as:
```
    y[i] ~ dnorm(alpha[eth[i]] + 
                  beta[eth[i]]*(x[i] - mean(x)), 
                    sigma^-2)
```
which in maths can be written as:
$$y_i \sim N(\alpha_{\mbox{eth}_i} + \beta_{\mbox{eth}_i} x_i, \sigma^2)$$
where $\mbox{eth}_i$ takes the values 1, 2, 3, or 4

- Remember, $y_i$ is the log-earnings of individual $i$ where $i=1,\ldots,N$

## Re-writing the model 

- Commonly you'll see $y$ here re-defined as $y_{ij}$ where $j=1,..,4$ represents ethnicity, and $i=1,\ldots,N_j$ is the number of individuals with ethnicity $j$
- The likelihood can then be written as:
$$y_{ij} \sim N(\alpha_j + \beta_j x_i, \sigma^2)$$
- Note that this is exactly the same model, just re-written slightly differently. In fact, this latter model is much harder to write out in JAGS/Stan code

## Fixed vs random effect models

- Thinking about this model in more detail
$$y_{ij} \sim N(\alpha_j + \beta_j x_i, \sigma^2)$$
- If the $\alpha_j$ and $\beta_j$ parameters are all given independent prior distributions, e.g. $\alpha_j \sim N(0, 100)$ or similar, then this is considered a _fixed effects_ model
- If the $\alpha_j$ and $\beta_j$ are given prior distributions that tie the values together, e.g. $\alpha_j \sim N(\mu_\alpha, \sigma_\alpha^2)$, then this is often called a _random effects_ model
- (In fact, nobody can agree on what a fixed or random effects model actually is)
- If you have multiple different types of priors then this is called a _mixed effects_ model

## Mixed effects vs hierarchical models

- The hierarchical models we have been studying all use the _random effects_ approach wherever possible
- The big advantage of using this approach is that we get to _borrow_ strength between the different groups (here `eth`, but it could be anything)
- Whenever we have a categorical covariate we should always be putting a constraining/tying prior distribution on them, and looking at how the effects vary between the groups
- This is strongly linked to the idea of _partial pooling_ which we will meet later in the course

## Example: multi-layer earnings data

- We will now go through and build a much more complicated model for the earnings data, taken from the Gelman and Hill book, using only weak priors
- We can generate data from these models (either using the prior or the posterior), and we can draw a DAG
- Our goal is to explore the factors which explain earnings. We have variables on height, age, and ethnicity.
- If we first plot the data
```{r, echo = FALSE, fig.height=4}
dat = read.csv('../data/earnings.csv')
par(mfrow=c(1,2))
plot(jitter(dat$height_cm), dat$earn, xlab = 'Height (cm)', ylab = 'Earnings ($)')
plot(jitter(dat$height_cm), log(dat$earn), xlab = 'Height (cm)', ylab = 'log(Earnings ($))')
par(mfrow=c(1,1))
```

## Transformations

- From the left-hand plot there seem to be quite a few extreme observations, and there's a possibility that the relationship between height and earnings is non-linear
- The right-hand plot seems to have stabilised most of the extreme observations, and perhaps linearity is more appropriate
- Notice that a linear model implies:
$$y_i = \alpha + \beta x_i + \epsilon_i$$
whilst the log-linear model implies:
$$y_i = \exp( \alpha + \beta x_i + \epsilon_i) = e^\alpha \times e^{\beta x_i} \times e^\epsilon_i$$
so the coefficients, once exponentiated, have multiplicative effects
that are relatively easy to interpret

## Fitting the first model

- If we fit a model with just height (mean centered) we get the following JAGS output

\tiny
```{r, include = FALSE}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha + beta_height*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  alpha ~ dnorm(0, 20^-2)
  beta_height ~ dnorm(0, 20^-2)
  sigma ~ dt(0,10,1)T(0,)
}
'
jags_run = jags(data = list(N = nrow(dat), 
                            log_earn = log(dat$earn),
                            height = dat$height_cm),
                parameters.to.save = c('alpha',
                                       'beta_height',
                                       'sigma'),
                model.file = textConnection(jags_code))
```
```{r, echo = FALSE}
print(jags_run)
```

## Interpreting the parameters

- These parameters are directly interpretable:

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
low = signif(exp(jags_summ['alpha','2.5%']),2)
high = signif(exp(jags_summ['alpha','97.5%']),2)
sig = jags_summ['sigma','mean']
beta = jags_summ['beta_height','mean']
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

    - The mean of the log earnings at the mean height is about 9.737, which is about 17k on the original scale
    - We can also get e.g. a 95% confidence interval using the JAGS output. From $`r format(low, scientific = FALSE)`$ to $`r format(high, scientific = FALSE)`$
    - For every extra cm so you gain $`r format(beta, scientific = FALSE, digits = 3)`$ on the log scale, i.e. an $`r format(100*exp(beta)-100, scientific = FALSE, digits = 3)`$% gain in income
    - From the posterior of $\sigma$, we can guess that about 68% of predictions will be within $`r format(sig, scientic = FALSE, digits = 3)`$ on the log scale or within a factor of about $`r format(exp(sig), scientic = FALSE, digits = 3)`$ of the prediction 
- Interpretation for the intercept would have been harder had we not mean-centered the height variable
- The DIC is $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters

## Improving the model

- Now suppose we fit a model with a random intercept for ethnicity

\small
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    alpha_eth[j] ~ dnorm(mu_eth, sigma_eth^-2)
  }
  beta_height ~ dnorm(0, 20^-2)
  mu_eth ~ dnorm(0, 20^-2)
  sigma_eth ~ dt(0,10,1)T(0,)
  sigma ~ dt(0,10,1)T(0,)
}
'
```

## Improving the model 2

```{r, message = FALSE, results = 'hide', echo = FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth),
                parameters.to.save = c('alpha_eth',
                                       'beta_height',
                                       'mu_eth',
                                       'sigma'),
                model.file = textConnection(jags_code))
```

\tiny
```{r, echo = FALSE}
print(jags_run)
```

## Interpreting the output

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
low = signif(exp(jags_summ['mu_eth','2.5%']),2)
high = signif(exp(jags_summ['mu_eth','97.5%']),2)
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

- The parameters $\alpha$ and $\beta_{\mbox{height}}$ haven't changed much in the mean 
- The 95% confidence interval for $\alpha$ has increased: $`r format(low, scientific = FALSE)`$ to $`r format(high, scientific = FALSE)`$
- The DIC is $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters. Pretty much the same as above
- We also have estimates for each ethnicity, none of these have a strong effect away from zero

## Now an interaction model

\tiny
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height[eth[i]]*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    alpha_eth[j] ~ dnorm(mu_eth, sigma_eth^-2)
    beta_height[j] ~ dnorm(mu_beta_height, sigma_height^-2)
  }
  mu_beta_height ~ dnorm(0, 20^-2)
  mu_eth ~ dnorm(0, 20^-2)
  sigma_eth ~ dt(0,10,1)T(0,)
  sigma_height ~ dt(0,10,1)T(0,)
  sigma ~ dt(0,10,1)T(0,)
}
'
```

## Interaction model results


```{r, include = FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth),
                parameters.to.save = c('alpha_eth',
                                       'beta_height',
                                       'beta_eth',
                                       'mu_eth',
                                       'mu_beta_height',
                                       'sigma_height',
                                       'sigma'),
                model.file = textConnection(jags_code))
```

\tiny
```{r, echo = FALSE}
print(jags_run)
```

## Interpreting the output

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
low = signif(exp(jags_summ['mu_eth','2.5%']),2)
high = signif(exp(jags_summ['mu_eth','97.5%']),2)
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

- The model has improved a bit, DIC now $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters
- The confidence intervals for the different slopes are highly different, with the whites group (ethnicity = 3) having a much clearer relationship with height, possibly due to the large sample size
- Go back to the previous classes to see plots of these effects

## Checking the model - posterior predictive fit

```{r, include = FALSE}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height[eth[i]]*(height[i] - mean(height)), 
                    sigma^-2)
    log_earn_pred[i] ~ dnorm(alpha_eth[eth[i]] + 
                    beta_height[eth[i]]*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    alpha_eth[j] ~ dnorm(mu_eth, sigma_eth^-2)
    beta_height[j] ~ dnorm(mu_beta_height, sigma_height^-2)
  }
  mu_beta_height ~ dnorm(0, 20^-2)
  mu_eth ~ dnorm(0, 20^-2)
  sigma_eth ~ dt(0,10,1)T(0,)
  sigma_height ~ dt(0,10,1)T(0,)
  sigma ~ dt(0,10,1)T(0,)
}
'
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth),
                parameters.to.save = c('log_earn_pred'),
                model.file = textConnection(jags_code))
```
```{r, echo = FALSE}
plot(log(dat$earn), jags_run$BUGSoutput$mean$log_earn_pred, xlab = 'True log(earnings)', ylab = 'Predicted log(earnings)')
abline(a = 0, b = 1, col = 'red')
```

## Introducing age

- Let's fit an even more complicated model with intercepts and slopes varying by ethnicity and age group

- Age is divided up into three groups 1: 18-34, 2: 35-49, and 3: 50-64

- We want to know whether the degree to which height affects earnings  for different ethnic/age group combinations

## JAGS model

\tiny
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(alpha[eth[i],age_grp[i]] +
                          beta[eth[i],age_grp[i]]*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    for(k in 1:N_age_grp) {
      alpha[j,k] ~ dnorm(mu_alpha, sigma_alpha^-2)
      beta[j,k] ~ dnorm(mu_beta, sigma_beta^-2)
    }
  }
  mu_alpha ~ dnorm(0, 20^-2)
  mu_beta ~ dnorm(0, 20^-2)
  sigma_alpha ~ dt(0,10,1)T(0,)
  sigma_beta ~ dt(0,10,1)T(0,)
  sigma ~ dt(0,10,1)T(0,)
}
'
```

## Model output

```{r, include = FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            N_age_grp = length(unique(dat$age)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth,
                            age_grp = dat$age),
                parameters.to.save = c('alpha',
                                       'beta'),
                model.file = textConnection(jags_code))
```
```{r, echo = FALSE}
pars = jags_run$BUGSoutput$mean
age_grp_names = c('18-34','35-49','50-64')
eth_names = c('Blacks','Hispanics','Whites','Others')

par(mfrow=c(4,3))
for(i in 1:4) {
  for(j in 1:3) {
    curr_dat = subset(dat, dat$eth == i & dat$age == j)
    plot(curr_dat$height_cm, log(curr_dat$earn), main = paste(eth_names[i], age_grp_names[j]), ylab = 'log(earnings)', xlab = 'Height (cm)')
    lines(dat$height_cm, pars$alpha[i,j] + pars$beta[i,j]*(dat$height_cm - mean (dat$height_cm)), col = i)    
  }
}
par(mfrow=c(1,1))
```

## More about this model

- So we now have varying effects - we should also plot the uncertainties in these lines (see practical)

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

- The DIC here is now DIC now $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters - a big drop!

- There is another way to make the model even more complicated ...

## Multivariate models

- Often we have more than one variable or parameter that changes simultaneously

- For example, we might be interested in how both earnings and social grade change in response to height. These are likely to be correlated, even after we take into account the effect of height

- Or we might be interested in how both the slope and the intercept change in a particular model

- In either case, the _multivariate normal distribution_ helps us achieve this by borrowing strength across the different dimensions

## The multivariate normal distribution

- The standard normal distribution we have met has two parameters, a mean and a standard deviation/variance
- The multivariate normal (MVN) distribution has the same, except that the mean is now a _vector_ and the variance is now a _matrix_ sometimes called the _covariance matrix_
- Each element of the mean represents the mean of each variable. Each diagonal element of the covariance matrix is the variance of that variable, and each off-diagonal element is the covariance between those two variables
- For the 2D MVN distribution, we write:
$$\left[ \begin{array}{c} y_1 \\ y_2 \end{array} \right] \sim N \left( \left[ \begin{array}{c} \mu_1 \\ \mu_2 \end{array} \right], \left[ \begin{array}{cc} v_{11} & v_{12} \\ v_{12} & v_{22} \end{array} \right] \right)$$
or, for short
$$y \sim MVN(\mu, \Sigma)$$

## Learning about the parameters of the MVN

- You can think of the MVN distribution as borrowing strength between variables, just like the hierarchical model borrows strength across categories
- If we want to use the MVN in a Bayesian model we need to be able to put prior distributions on the parameters
- The means $\mu$ are easier as we can set independent prior distributions on them, e.g. $\mu_1 \sim N(0, 100)$
- The covariance matrix is more fiddly. Luckily there is a probability distribution called the _Wishart distribution_ which works on covariance matrices. These are especially tricky as the covariances and the variances have to match appropriately

## Returning to the earnings data

- Let's add to our complicated model with intercepts and slopes varying by ethnicity and age group

- We will include a multivariate normal prior on the intercept/slope pairs and look at the interaction between ethnicity and ages

- We are introducing a specific parameter which measures the degree of correlation between the intercept and the slope for each group. This could potentially vary between age group or ethnicity

## Fitting a multivariate, multi-layer model

\tiny
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    log_earn[i] ~ dnorm(beta[eth[i],age_grp[i],1] +
                          beta[eth[i],age_grp[i],2]*(height[i] - mean(height)), 
                    sigma^-2)
  }
  # Priors
  for(j in 1:N_eth) {
    for(k in 1:N_age_grp) {
      beta[j,k,1:2] ~ dmnorm(mu_beta[1:2,1], Sigma_beta_inv)
    }
  }
  for(l in 1:2) {
      mu_beta[l,1] ~ dnorm(0, 20^-2)
  }
  Sigma_beta_inv ~ dwish(R_beta,k_beta)
  sigma ~ dt(0,10,1)T(0,)
}
'
```

## Running the model

\tiny
```{r, include = FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            N_eth = length(unique(dat$eth)),
                            N_age_grp = length(unique(dat$age)),
                            log_earn = log(dat$earn),
                            height = dat$height_cm,
                            eth = dat$eth,
                            age_grp = dat$age,
                            k_beta = 2,
                            R_beta = diag(2)),
                parameters.to.save = c('beta'),
                model.file = textConnection(jags_code))
```
```{r}
print(jags_run)
```

## Did the model fit the data any better?

```{r, include = FALSE}
jags_summ = jags_run$BUGSoutput$summary
DIC = round(jags_run$BUGSoutput$DIC,2)
pD = round(jags_run$BUGSoutput$pD,2)
```

- The DIC here is now DIC now $`r format(DIC,scientific=FALSE)`$ with $`r format(pD,scientific=FALSE)`$ effective parameters - about the same as before?

- We could also watch the parameters `mu_beta` and `sigma_beta_inv`. The latter will tell us about the correlation structure between the intercepts and the slopes

- We could make the model even more complicated by allowing the multivariate `mu_beta` parameters to vary by age group, ethnicity, and their interaction

## Plots of output

```{r, echo = FALSE}
pars = jags_run$BUGSoutput$mean
age_grp_names = c('18-34','35-49','50-64')
eth_names = c('Blacks','Hispanics','Whites','Others')

par(mfrow=c(4,3))
for(i in 1:4) {
  for(j in 1:3) {
    curr_dat = subset(dat, dat$eth == i & dat$age == j)
    plot(curr_dat$height_cm, log(curr_dat$earn), main = paste(eth_names[i], age_grp_names[j]), ylab = 'log(earnings)', xlab = 'Height (cm)')
    lines(dat$height_cm, pars$beta[i,j,1] + pars$beta[i,j,2]*(dat$height_cm - mean (dat$height_cm)), col = i)    
  }
}
par(mfrow=c(1,1))
```

## Missing and unbalanced data

- There are many definitions of what 'unbalanced' data means in statistics. Usually we mean that there are different numbers of observations in each group. Our format of writing e.g. $y_i \sim N(\alpha_{\mbox{eth}_i} + \beta_{\mbox{eth}_i} x_i, \sigma^2)$ allows us to deal with unbalanced data naturally

- Usually the smaller the sample size of the group the more uncertain the posterior distribution will be

- But what if we have some missing data? There are different types, and some need to be more carefully treated than others

## Different types of missing data

- There are many different types of missing data:

    - Missing response variables
    - Missing covariates
    - Missingness that occurs completely at random
    - Missingness that occurs as a consequence of the experiment or the data
    
- The first three are all very easy to deal with in JAGS (less so in Stan). The last one is much harder, and not something we will go into in any detail. It requires building a separate model for the missingness process

## The simple way of dealing with missing data in JAGS

- In JAGS it is absolutely trivial to to deal with missingness in the response variable. You simply fill in the missing values with `NA`
- JAGS then treats them as parameters to be estimated. You can 'watch' them in the normal way or just ignore them. You thus have the option of getting a posterior distribution of the missing data points
- Suppose we shoved in some NA values into our data
```{r}
dat2 = dat
dat2$earn[c(177, 763, 771)] = NA
```

## Running the model with missingness

\tiny
```{r, include = FALSE}
jags_run = jags(data = list(N = nrow(dat2), 
                            N_eth = length(unique(dat2$eth)),
                            N_age_grp = length(unique(dat2$age)),
                            log_earn = log(dat2$earn),
                            height = dat2$height_cm,
                            eth = dat2$eth,
                            age_grp = dat2$age,
                            k_beta = 2,
                            R_beta = diag(2)),
                parameters.to.save = c('log_earn[177]','log_earn[763]','log_earn[771]'),
                model.file = textConnection(jags_code))
```
```{r}
print(jags_run)
```

## More complex varieties of missing data

- If you have missing covariates or joint missing covariates/response variables, you can include these too
- The only extra issue is that you need to give JAGS a prior distribution for the missing covariate values which can make the code a bit fiddlier
- If the response variable (e.g. log earnings) exists but the covariate value is missing, then you are asking JAGS to perform an _inverse regression_
- In Stan missing data is fiddlier to incorporate as you have to separate out the parameters (i.e. missing data) from the observed data

## Summary

- We have seen how to create some rich multi-layers models
- We have gone through quite a detailed example
- We have learnt about the multivariate normal distribution
- We have discovered how to deal with missing and unbalanced data sets
